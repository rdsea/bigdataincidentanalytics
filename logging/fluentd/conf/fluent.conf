# fluentd/conf/fluent.conf
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>
<source>
  @type http
  port 9880
  bind 0.0.0.0
  cors_allow_origins ["*"]
  <parse>
    @type json
    time_key time
    keep_time_key true
    time_type string
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Filter for concatenating multi-line stack-traces
<filter docker.**>
  @type concat
  key log
  stream_identity_key container_id
  multiline_start_regexp /^([0-9]{4}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1])( |T)(2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9])|([0-9]{2} [A-z]{3} (2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9])|([0-9]{2}\/(0[1-9]|1[0-2])\/(0[1-9]|[1-2][0-9]|3[0-1]) (2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9])/
  separator ""
  flush_interval 0
</filter>

<filter docker.mqtt>
  @type record_modifier
  <record>
    pipeline_component MQTT_BROKER
    message ${record['log']}
  </record>
</filter>

# ALL LOGS COMING WITH "docker.*" TAGS ARE CONSIDERED TO BE PLATFORM LOGS
<filter {docker.**,**.platform**}>
  @type record_modifier
  <record>
    software PLATFORM
  </record>
</filter>

# ALL LOGS WITH TAGS ENDING WITH "app" ARE CONSIDERED TO BE APPLICATION LOGS
<filter **.app.**>
  @type record_modifier
  <record>
    software APPLICATION
  </record>
</filter>

# JVM Logback based platform logs emit the log line via "msg" key, we
# modify the key to "log" for uniformity
<filter {**flink.platform**,**nifi.platform**,**spark.platform**}>
  @type record_modifier
  <record>
    log ${record['msg']}
  </record>
  remove_keys msg
</filter>

# FILTERS FOR SENSORS
<filter **.sensor.**>
  @type record_modifier
  <record>
    pipeline_component SENSOR
  </record>
</filter>
<filter **.sensor.**.dataAsset>
  @type record_modifier
  <record>
    asset_state IN_MOTION
    function DATA_EXTRACTION_INGESTION
  </record>
</filter>

# FILTERS FOR NODE-RED
<filter **.nodered.**>
  @type record_modifier
  <record>
    pipeline_component NODE-RED
  </record>
</filter>
<filter **.nodered.**.dataAsset>
  @type record_modifier
  <record>
    asset_state AT_REST
    function DATA_STORAGE
  </record>
</filter>

# FILTERS FOR FLINK
<filter {**.flink**,**jobmanager**}>
  @type record_modifier
  <record>
    pipeline_component FLINK
  </record>
</filter>
<filter **.flink-processing.mqtt.*.dataAsset>
  @type record_modifier
  <record>
    asset_state IN_PROCESSING
    function DATA_LOADING_PREPROCESSING
  </record>
</filter>
<filter **.flink-processing.aggregation.*.dataAsset>
  @type record_modifier
  <record>
    asset_state IN_PROCESSING
    function DATA_PROCESSING
  </record>
</filter>
<filter **.flink-processing.storage.*.dataAsset>
  @type record_modifier
  <record>
    asset_state IN_MOTION
    function DATA_EXTRACTION_INGESTION
  </record>
</filter>

# FILTERS FOR NIFI
<filter **.nifi.**>
  @type record_modifier
  <record>
    pipeline_component NIFI
  </record>
</filter>
<filter **.nifi.**.dataAsset>
  @type record_modifier
  <record>
    asset_state IN_PROCESSING
    function DATA_LOADING_PREPROCESSING
  </record>
</filter>
<filter **.nifi.**.error>
  @type record_modifier
  <record>
    incident unavailable service
    function DATA_LOADING_PREPROCESSING
    effect REDUCTION_OF_QUALITY
  </record>
</filter>

# FILTERS FOR SPARK
<filter **.spark**>
  @type record_modifier
  <record>
    pipeline_component SPARK
  </record>
</filter>
<filter **.spark-kmeans.**.dataAsset>
  @type record_modifier
  <record>
    asset_state IN_PROCESSING
    function DATA_ANALYSIS
  </record>
</filter>
<filter **.spark-kmeans.**.error>
  @type record_modifier
  <record>
    incident unavailable service
    function DATA_ANALYSIS
    effect UNPLANNED_INTERRUPTION
  </record>
</filter>

<filter {**namenode**,**datanode**,**resourcemanager**,**nodemanager**}>
  @type parser
  key_name log
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^(\d{2}\/\d{2}\/\d{2}\s+[\d|\:]+)\s+(?<level>\S+)\s(?<logger>\S+):\s(?<message>[\s|\S]+)$/
  </parse>
</filter>
<filter {**namenode**,**datanode**,**resourcemanager**,**nodemanager**}>
  @type record_modifier
  <record>
    pipeline_component HADOOP
  </record>
</filter>

<label @FLUENT_LOG>
  <match fluent.*>
    @type stdout
  </match>
</label>

@include mqtt_rules.conf

<match **signal>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix fluentd
    logstash_dateformat %Y%m%d
    include_tag_key true
    include_timestamp true
    type_name _doc
    tag_key @log_name
    default_elasticsearch_version 6.5.4
    <buffer tag>
      @type memory # or file
      flush_thread_count 4
      flush_interval 2s
    </buffer>
  </store>
  <store>
    @type file
    path /fluentd/log/bigdataincidentanalytics/pipeline/master.%Y-%m-%d.%H
    append true
  </store>
  #@include kafka_output.conf
</match>

<match *.**>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix fluentd
    logstash_dateformat %Y%m%d
    include_tag_key true
    include_timestamp true
    type_name _doc
    tag_key @log_name
    default_elasticsearch_version 6.5.4
    <buffer tag>
      @type memory # or file
      flush_thread_count 4
      flush_interval 2s
    </buffer>
  </store>
</match>
