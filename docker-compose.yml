version: '3.4'

x-fluentd-logging:
  &custom-logging
  logging:
    driver: fluentd
    options:
      fluentd-address: localhost:24224
      tag: docker.{{.Name}}

services:

  ##### IoT SENSORS #####
  simplesensor:
    image: danielfuvesi/bigdataincidentanalytics:simplesensor
    networks:
      - iot-big-data
    volumes:
      - ./pipeline/sensors/simplesensor:/var/config
      - $ETC_LOCALTIME:/etc/localtime:ro
    command: npm start -- -c /var/config/config.json
    << : *custom-logging

  ##### MQTT BROKER #####
  mqtt:
    image: eclipse-mosquitto
    container_name: mqtt
    domainname: iot-big-data
    networks:
      - iot-big-data
    ports:
      - 1883:1883
      - 9001:9001
    volumes:
      - $ETC_LOCALTIME:/etc/localtime:ro
    << : *custom-logging

  ##### APACHE FLINK #####
  flink_jobmanager:
    build: pipeline/flink
    container_name: jobmanager
    expose:
      - 6123
      - 9249
    ports:
      - 8081:8081
    volumes:
      - ./pipeline/flink/logback.xml:/opt/flink/conf/logback-console.xml:ro
      - $ETC_LOCALTIME:/etc/localtime:ro
    networks:
      - iot-big-data
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      - LOG_COMPONENT_TAG=flink.platform.jobmanager

  flink_taskmanager:
    build: pipeline/flink
    expose:
      - 6121
      - 6122
      - 9249
    volumes:
      - ./pipeline/flink/logback.xml:/opt/flink/conf/logback-console.xml:ro
      - $ETC_LOCALTIME:/etc/localtime:ro
    networks:
      - iot-big-data
    depends_on:
      - flink_jobmanager
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      - FLUENTD_TAG_PREFIX=flink-processing
      - FLUENTD_HOST=fluentd
      - FLUENTD_PORT=24224
      - LOG_COMPONENT_TAG=flink.platform.taskmanager

  ##### APACHE NIFI #####
  nifi:
    build: pipeline/nifi
    hostname: nifi
    container_name: nifi
    volumes:
      - ./pipeline/nifi/logs:/opt/nifi/nifi-current/logs
      - ./pipeline/hadoop/config:/opt/nifi/nifi-current/hadoop
      - ./pipeline/nifi/logback.xml:/opt/nifi/nifi-current/conf/logback.xml:ro
      - $ETC_LOCALTIME:/etc/localtime:ro
    networks:
      - iot-big-data
    ports:
      - 8080:8080
    environment:
      - LOG_COMPONENT_TAG=nifi.platform

  ##### APACHE HADOOP #####
  namenode:
    image: uhopper/hadoop-namenode
    hostname: namenode
    container_name: namenode
    expose:
      - 8020
    networks:
      - iot-big-data
    volumes:
      - /hadoop/dfs/name
      - ./pipeline/hadoop/hadoop_setup.sh:/usr/local/bin/hadoop_setup.sh
      - $ETC_LOCALTIME:/etc/localtime:ro
    command: hadoop_setup.sh
    environment:
      - CLUSTER_NAME=myCluster
    ports:
      - 50070:50070
    << : *custom-logging

  datanode1:
    image: uhopper/hadoop-datanode
    hostname: datanode1
    container_name: datanode1
    networks:
      - iot-big-data
    volumes:
      - /hadoop/dfs/data
      - $ETC_LOCALTIME:/etc/localtime:ro
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_permissions=false
    << : *custom-logging

  resourcemanager:
    image: uhopper/hadoop-resourcemanager
    hostname: resourcemanager
    container_name: resourcemanager
    networks:
      - iot-big-data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_log___aggregation___enable=true
      - HDFS_CONF_dfs_permissions=false
    ports:
      - 8088:8088
    volumes:
      - $ETC_LOCALTIME:/etc/localtime:ro
    << : *custom-logging

  nodemanager1:
    image: uhopper/hadoop-nodemanager
    hostname: nodemanager1
    container_name: nodemanager1
    networks:
      - iot-big-data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_log___aggregation___enable=true
      - YARN_CONF_yarn_nodemanager_remote___app___log___dir=/app-logs
      - HDFS_CONF_dfs_permissions=false
    volumes:
      - $ETC_LOCALTIME:/etc/localtime:ro
    << : *custom-logging

  ##### Node-RED #####
  nodered:
    image: nodered/node-red-docker:slim
    ports:
      - 1880:1880
    networks:
      - iot-big-data
    volumes:
      - ./pipeline/nodered:/data
      - $ETC_LOCALTIME:/etc/localtime:ro
    environment:
      - FLUENTD_TAG_PREFIX=nodered
      - FLUENTD_HOST=fluentd
      - FLUENTD_PORT=24224

  ##### ELK STACK #####
  elasticsearch:
    build:
      context: pipeline/elk/elasticsearch/
      args:
        ELK_VERSION: $ELK_VERSION
    container_name: elasticsearch
    volumes:
      - ./pipeline/elk/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
      - $ETC_LOCALTIME:/etc/localtime:ro
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      ES_JAVA_OPTS: "-Xmx256m -Xms256m"
    networks:
      - iot-big-data

  kibana:
    build:
      context: pipeline/elk/kibana/
      args:
        ELK_VERSION: $ELK_VERSION
    container_name: kibana
    volumes:
      - ./pipeline/elk/kibana/config/:/usr/share/kibana/config:ro
      - $ETC_LOCALTIME:/etc/localtime:ro
    ports:
      - 5601:5601
    networks:
      - iot-big-data
    depends_on:
      - elasticsearch

  ##### APACHE SPARK #####
  spark-master:
    build: pipeline/spark/master
    container_name: spark-master
    ports:
      - 8082:8080
      - 7077:7077
    environment:
      - ENABLE_INIT_DAEMON=false
      - "constraint:node==master"
      - LOG_COMPONENT_TAG=spark.platform.master
    volumes:
      - ./pipeline/hadoop/config/core-site.xml:/etc/hadoop/core-site.xml
      - ./pipeline/hadoop/config/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      - ./pipeline/spark/config/spark-env.sh:/spark/conf/spark-env.sh
      - ./pipeline/spark/master/logback.xml:/spark/conf/logback.xml:ro
      - $ETC_LOCALTIME:/etc/localtime:ro
    networks:
      - iot-big-data

  spark-worker:
    build: pipeline/spark/worker
    depends_on:
      - spark-master
    ports:
      - 8081
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - ENABLE_INIT_DAEMON=false
      - "constraint:node==master"
      - LOG_COMPONENT_TAG=spark.platform.worker
    volumes:
      - ./pipeline/hadoop/config/core-site.xml:/etc/hadoop/core-site.xml
      - ./pipeline/hadoop/config/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      - ./pipeline/spark/config/spark-env.sh:/spark/conf/spark-env.sh
      - ./pipeline/spark/worker/logback.xml:/spark/conf/logback.xml:ro
      - $ETC_LOCALTIME:/etc/localtime:ro
    networks:
      - iot-big-data

  spark-app:
    build: pipeline/spark/iot-big-data-spark
    container_name: spark-app
    ports:
      - 4040
    depends_on:
      - spark-master
      - spark-worker
    environment:
      - ENABLE_INIT_DAEMON=false
      - SPARK_MASTER_NAME=spark-master
      - SPARK_MASTER_PORT=7077
      - FLUENTD_TAG_PREFIX=spark-kmeans
      - FLUENTD_HOST=fluentd
      - FLUENTD_PORT=24224
      - HDFS_URL=hdfs://namenode:8020/nifi
      - LOG_COMPONENT_TAG=spark.platform.kmeans
    volumes:
      - $ETC_LOCALTIME:/etc/localtime:ro
      - ./pipeline/spark/iot-big-data-spark/logback.xml:/spark/conf/logback.xml:ro
    networks:
      - iot-big-data

  # THIS SERVICE IS DEFINED JUST FOR COMPLETENESS. FLUENTD *MUST* BE STARTED ON HOST BEFORE ANY OTHER SERVICE,
  # OTHERWISE THE OTHER CONTAINERS CAN'T START DUE TO MISSING LOGGING DRIVER.
  fluentd:
    build: logging/fluentd
    container_name: fluentd
    volumes:
      - ./logging/fluentd/conf:/fluentd/etc
      - ./logging/fluentd/logs:/fluentd/log/bigdataincidentanalytics
      - ./logging/fluentd/logs/pipeline:/fluentd/log/bigdataincidentanalytics/pipeline
      - $ETC_LOCALTIME:/etc/localtime:ro
    networks:
      - iot-big-data
    ports:
      - 24224:24224
      - 24224:24224/udp
      - 9880:9880

  # Just for showcasing different situations regarding monitoring and reasoning
  neo4j:
    build:
      context: monitoring/constraints/
      args:
        CYPHER_FILE: database.cypher
    container_name: neo4j
    restart: always
    networks:
      - iot-big-data
    ports:
      - 7474:7474
      - 7687:7687

networks:
  iot-big-data:
    driver: bridge