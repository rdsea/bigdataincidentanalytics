# fluentd/conf/fluent.conf
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>
#<match docker.**>
#  @type detect_exceptions
#  remove_tag_prefix docker
#  multiline_flush_interval 1
#</match>
<filter docker.**>
  @type concat
  key log
  stream_identity_key container_id
  multiline_start_regexp /^([0-9]{4}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1])( |T)(2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9])|([0-9]{2} [A-z]{3} (2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9])|([0-9]{2}\/(0[1-9]|1[0-2])\/(0[1-9]|[1-2][0-9]|3[0-1]) (2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9])/
  separator ""
  flush_interval 10
</filter>
<match *.**>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix fluentd
    logstash_dateformat %Y%m%d
    include_tag_key true
    include_timestamp true
    type_name fluentd
    tag_key @log_name
    default_elasticsearch_version 6.5.4
    <buffer tag>
      @type memory # or file
      flush_thread_count 4
      flush_interval 2s
    </buffer>
  </store>
  <store>
    @type file
    path /fluentd/log/bigdataincidentanalytics
    compress gzip
    append true
    time_slice_wait 1m
  </store>
</match>
